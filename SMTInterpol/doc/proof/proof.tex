\documentclass[a4paper]{article}
\usepackage{xspace}
\usepackage{relsize}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{pxfonts}
\usepackage{mathpartir}
\newcommand\mT{\mathcal{T}}
\newcommand\T{$\mT$\xspace}
\newcommand\Tp{$\mT'$\xspace}
\newcommand\mtp{\models_{\mT'}}
\newcommand\syms{\mathop{\mathit{syms}}}
\newcommand\si{SMTInterpol\xspace}
\newcommand\sversion{2.0}
\newcommand\siv{\si~\sversion}
\newcommand\gen[1]{\mathop{gen}\nolimits_{#1}}
\newcommand\dom{\mathop{dom}}
\newcommand\todo[1]{\textcolor{red}{TODO: #1}}
\newcommand\seqcomp{\circledwedge}
\newcommand\choicecomp{\circledvee}

%canonical form
\newcommand\cf{\mathit{cf}}
\newcommand\annot[2]{(!\ #1\ \texttt{#2})}

\title{Proposal: Proof System for \siv}
\author{J{\"u}rgen Christ \and Jochen Hoenicke}
\date{2017/09/27}
\begin{document}
\maketitle
\section{Motivation}
With the increase of complexity of the SMT solver \si, soundness gets a major
concern.  For satisfiable formulas, the solver can generate a model that
should satisfy all input formulas.  Checking this becomes hard when the input
contains quantifiers.  For unsatisfiable formulas, the solver can deliver a
proof of unsatisfiability, i.\,e., a derivation of \texttt{false} from the
input formulas.  This proof should contain all the steps needed to show
inconsistency of the current set of formulas.  To achieve this, a proof system
has to be given.  This system, however, should be as close to the operations
of the solver as possible.  Unfortunately, tracking every step of the solver
becomes challenging as the resulting proof might quickly grow to a size that is
too big to keep in memory.

In this document, we propose a proof system that completely captures
the inferences done by \si at an adequate level while keeping a
reasonably small memory overhead.  The main design goals of the system
are easy integration into \si and the possibility to reconstruct the
proof in another verification system, e.\,g., Isabelle.

\section{Proof System in \si}

The proof is done in three layers.  The first layer transforms input
clauses into a normal form, e.g. reducing the number of built-in
functions and simplifying some terms. This layer uses only equality
and equivalence rewrites on the whole formula and produces an
equivalent formula.

The second layer splits the formula into clauses and introduces
tautologies to define auxiliary literals.

The third and outermost layer is a resolution-based proof that uses
the input clauses and theory lemmas to proof the empty clause.
This concludes the proof.

In this section we present the three layers, starting with the
outermost layer.

\subsection{Resolution Proofs}

A resolution proof is encoded using a resolution function \verb+@res+, that
takes as input two proofs for two clauses $(or\ \ell\ C_1)$ and
$(or\ (not\ \ell)\ C_2)$ and returns a proof for the clause $(or\ C_1\ C_2)$.
The resolution function is
left-associative and has the following  signature.

\begin{verbatim}
:funs ((@res @Proof @Proof @Proof :left-assoc))
\end{verbatim}

To quickly find the pivot literal $\ell$ of the resolution, the second
proof argument is annotated with this literal.  This pivot literal
occurs negative in the first (the one not containing the pivot
annotation) and positive in the second argument.  If $\ell$ is a
negated literal, then in its negation the double negation is
implicitly removed.  The pivot literal $\ell$ may occur at any
position in the disjunction.  The remaining literals $C_1,C_2$ are
seen as sets, in particular, duplicated literals are automatically
omitted.  When clauses are printed as SMTLIB term, the function $or$
is only used for clauses of length $n\geq 2$.  Unit clauses are
represented by their literal and the empty clause by $false$.

Resolution proofs returned by \si are clean, i.e., no clause contains
a literal and its negation, and for every resolution step the pivot
literal really needs to occur in the input clauses.  Although, the
proven formula would still be valid without these restrictions, the
interpolation procedure require this form.

Since the proofs in \si were originally only constructed to produce
interpolants, there is a feature to cut the proof at the CNF-level.
To compute interpolants the rewrite steps that are local to a single
input formula are not needed.  Furthermore, for unsatisfiable cores
only the used input formulas are needed, not their transformation into
clauses.  Since resolution proofs are sufficient for these two
techniques, the proof system keeps this separation.  That way, \si
won't have performance penalties on these techniques.  If, however, a
user wants to retrieve a complete proof, the conversion of the
original formula into CNF is also tracked.

The partial proof system after CNF conversation consists of only three
rules: clauses created from input formulas (clause), theory lemmas
(lemma), and resolution steps (res).  The first two are the leaves of
the partial proof system and introduce valid clauses.  The resolution
rule represent the inner nodes of the proof tree.

When tracking the CNF conversation, the \verb+@clause+ function takes
as first argument, another proof that shows that the clause follows
from one of the input formulas (or from a tautology in case of Tseitin
encoded literals).  The second argument gives the resulting clause, so
that the proof can be reconstructed without following the subproof.
Theory lemmas introduced by \verb+@lemma+ are specialized to the
theories supported by \si.  They don't have a subproof, but they take
some auxiliary annotations that make understanding their validity
easier.

\subsection{Formula Transformation and Simplification}

This section describes the proof steps to normalize the input formula.
This proof uses rules based on equality rewrites to show the
equivalence of the input formula with a simpler formula.  Then the
equality rule is used to derive the simpler formula from the input
formula.

\paragraph{Equality rule.}  This proof rule takes a Boolean term\footnote{We treat
  formulas as Boolean terms.} and an equality and produces a new term
where some occurrences of the left hand side of the equality are
replaced by the right hand side.
\[
\inferrule*[left=eq]{t_1 \\ t_1=t_2}{t_2}
\]

The eq rule is represented in SMTLIB as a function
\begin{verbatim}
:funs ((@eq @Proof @Proof @Proof :left-assoc))
\end{verbatim}
This function takes a proof for $t_1$ and a proof for $t_1=t_2$ and
returns a proof for $t_2$.  The proved term $t_2$ is not provided but
can be easily computed by following the proof.

\paragraph{Lifting equalities.}  To rewrite only parts of a formula
\si uses the \textsc{congruence} and \textsc{exists} rule to lift
equalities from subterms.  Since all logical connectives in SMT-LIB
with the exception of quantifiers are Boolean functions, the
congruence rule can also be used for them.  Further there is the
\textsc{trans}itivity rule to combine rewrites.

\[
\inferrule*[left=trans]{t_1 = t_2 \\ t_2 = t_3}{t_1 = t_3}\qquad
\inferrule*[left=cong]{t = f(\dots t_i \dots) \\ t_i = t'_i}{t = f(\dots t'_i \dots)}\qquad
\inferrule*[left=exists]{F = G}{\exists x\ F = \exists x\ G}
\]

These rules are represented in SMTLIB as functions
\begin{verbatim}
:funs ((@trans @Proof @Proof @Proof :left-assoc)
       (@cong  @Proof @Proof @Proof :left-assoc)
       (@exists @Proof @Proof)
       (par (A) @refl A @Proof))
\end{verbatim}

The function \verb+@refl+ takes as argument a term $t$ and returns a
proof for $t=t$.  It is mainly used in the first argument of
\verb+@cong+ to rewrite a function application.  The functions
\verb+@trans+ and \verb+@cong+ take as arguments two proofs for the
terms as specified in the rules above and return a proof for the
resulting goal, which can easily be computed.  The function
\verb+@exists+ takes a proof for $F=G$.  To reconstruct the variables
\verb+((Sort x_1) ... (Sort x_n))+ they are added as an annotation to
the argument.  It returns the proof for the exists equality.

\paragraph{Rewrite axioms.}  The \textsc{rewrite} axiom introduces a
new equality $t_1=t_2$ where side conditions for $t_1$ and $t_2$
ensure that the equality is true.  They are introduced by a function
\verb+@rewrite+ that takes the equality $t_1=t_2$ and
returns a proof of the equality.

\begin{verbatim}
:funs ((@rewrite bool @Proof))
\end{verbatim}

The equality is annotated by the rewrite rule, which explains why
$t_1$ and $t_2$ should be equal. In the proof tree they appear as

\begin{verbatim}
  (@rewrite (! (= t1 t2) :rewrite-rule))
\end{verbatim}

where \texttt{:rewrite-rule} is the name of one of the rules listed in
the remaining section.


\paragraph{Expanding definitions.}  The rule \texttt{:expandDef} justifies
the expansion of a defined function.  Let $f(\overline{t})$ be the
application of $\overline{t}$ to the function named $f$, and
$d[\overline{t}]$ be the definition of $f$ where the free variables are
substituted by the corresponding terms.  Then, this rule justifies the
replacement of $n(\overline{t})$ by its definition.

\begin{equation}
f(\overline{t}) = d[\overline{t}]
\text{ where $d$ is the defintion of $f$} \tag{expandDef}
\end{equation}

Some internal functions are also defined as defined functions,
e.\,g., the $abs\ x$ function is defined as $(ite\ (>=\ x\ 0)\ x\ (-\ x))$. Below is an example proof using this definition.
\begin{verbatim}
(@rewrite (! (= (abs (- z)) (ite (>= (- z) 0) (- z) (- (- z))))
             :expandDef))
\end{verbatim}

\paragraph{Expanding $n$-ary function applications.}  \si expands function
applications that use associativity, chainability, or the pairwise attribute.
The expansion transforms an application of a left-associative function $f$ as follows.

\begin{equation*}
  (f\ t_1\ t_2 \cdots t_n) = (f\ (\ldots (f\ t_1\ t_2)\ \ldots)\ t_n)
  \text{ where $n > 2$, $f$ left-assoc.}
  \tag{expand}
\end{equation*}
Similarly, for a right associative function,
\begin{equation*}
  (f\ t_1\ t_2 \cdots t_n) = (f\ t_1\ \ldots (f\ t_{n-1}\ t_n)\ldots)
  \text{ where $n > 2$, $f$ right-assoc.}
  \tag{expand}
\end{equation*}
Finally for a chainable predicate,
\begin{equation*}
  (p\ t_1\ t_2 \cdots t_n) = (and\ (p\ t_1\ t_2) (p\ t_2\ t_3) \ldots (p\ t_{n-1}\ t_n))
  \text{ where $n > 2$, $p$ chainable.}
  \tag{expand}
\end{equation*}

Note that \si does not expand all such functions; for example, it
keeps $n$-ary disjunctions, conjunctions, and implications.  Sometimes
it even does the opposite using the flattening rule.

\paragraph{Flattening}
For an associative builtin operators $f$, i.e., when $(f\ (f\ a\ b)\ c) =
(f\ a\ (f\ b\ c))$ holds, the reverse rule of expand called (flatten),
removes nested application of $f$.  This rule works recursively on the
nested applications, but may leave some application of $f$ nested.
\begin{equation}
  (f\ (f\ t_1 \cdots) \cdots (f\ (f\ \cdots)) \cdots t_n) = (f\ t_1\cdots t_n)
  \text{ where $f \in \{and,or,xor,+,*\}$}
  \tag{flatten}
\end{equation}
Here the $f$ terms on the left-hand side can be nested arbitrarily
deep, the $t_i$ must appear in the same order on the right-hand side
and a $t_i$ can itself be an $f$ term again.

In principle a similar rule that supports only one level of nesting
would be sufficient, as it can be applied multiple times.  However the
corresponding proof DAG would be quadratic in the number of arguments
of the function, even with maximum sharing between identical sub
terms.  For example in $(f\ t_1\ (f\ t_2 \dots (f\ t_{n-1}\ t_n)))$, for
the $i$-th rewrite step a new term with $O(i)$ arguments would be
created, so this method would creates $O(n)$ terms of size $O(n)$
leading to a proof DAG of size $O(n^2)$.  This leads to considerable
performance bottlenecks for some benchmarks that contains ten
thousands conjuncts parenthesised in the above way.  Using the above
rule that allows arbitrary nesting, would flatten the formula in one
step with complexity $O(n)$.

\paragraph{Removal of Boolean connectives.}  Before producing clauses, \si
converts the formula into an equivalent form with less Boolean connectives.
In particular it uses only $not$, $or$, $ite$ and $xor$ as Boolean connectives.
These conversion steps are justified by the following rewrite rules
\begin{align}
  (and\ \cdots t_i\cdots) &=
  (not\ (or\ \cdots (not\ t_i)\cdots))
  \tag{andToOr}\\
  (=>\ \cdots t_i\cdots t_n) &=
  (or\ \cdots (not\ t_i)\cdots t_n))
  \tag{impToOr}
\end{align}

The function $distinct$ is rewritten to not equals in a later step.

\paragraph{Equality simplification.}
This set of rules justifies the simplification of an $n$-ary equality.  The
result depends on the simplification.  In general, it will be an equality with
up to $n$ arguments or a single term.  \si currently does the following
equality simplifications.
\begin{itemize}
\item Remove duplicated terms from the equality.  If the equality only
  contains duplicates, simplify to \verb+true+.
\item Simplify numeric equalities with different constants.
\item Simplify Boolean equalities:
  \begin{itemize}
  \item Simplify equalities containing \verb+true+ and \verb+false+ to
    \verb+false+.
  \item Simplify equalities containing \verb+true+ into conjunctions.
  \item Simplify equalities containing \verb+false+ into disjunctions.
  \item Finally convert binary Boolean equality to \verb+xor+.
  \end{itemize}
\end{itemize}

Equality simplification is split into several rules to ease proof validation.
\begin{align}
  (=\ t_1 \cdots t_n) &= false
  \quad \text{where some $t_j=true$ and some $t_k=false$}
  \tag{trueNotFalse}\\
  (=\ t_1 \cdots t_n) &= false
  \quad \text{where some $t_j$ and $t_k$ are distinct numeric constants}
  \tag{constDiff}\\
  (=\ t_1 \cdots t_n) &= (not\ (or\ \dots(not\ t_{i'})\dots))
  \quad \text{\begin{tabular}{l}where some $t_j = true$ and
      the $t_i'$ \\
      are the terms in the same order\\
      with $true$ and duplicates removed
    \end{tabular}}
  \tag{eqTrue}\\
  (=\ t_1 \cdots t_n) &= (not\ (or\ \dots t_{i'} \dots))
  \quad \text{\begin{tabular}{l}where some $t_j = false$ and
      the $t_i'$ \\
      are the terms in the same order\\
      with $false$ and duplicates removed
    \end{tabular}}
  \tag{eqFalse}\\
  (=\ t \cdots t) &= true
  \quad\text{where all $t$ are the same}
  \tag{eqSame}\\
  (=\ t_1 \cdots t_n) &= (=\ \dots t_{i'} \dots)
  \quad\text{\begin{tabular}{l}where $t_{i'}$ are the terms in the same\\
      order with repeated terms removed\end{tabular}}
  \tag{eqSame}\\
  (=\ t_1 \cdots t_n) &= (not\ (or\ (not\ (=\ t_1\ t_2)\dots(not\ (t_{n-1}\ t_n))
  \quad\text{where $n > 2$}
  \tag{eqBinary}\\
  (=\ t_1\ t_2) &= (not\ (xor\ t_1\ t_2)) \quad\text{where $t_1,t_2$ are Boolean}
  \tag{eqToXor}
\end{align}

For (eqTrue) resp. (eqFalse), in the case that there is only one
$t_i'$ the equality is rewritten to $t_i'$ resp. $(not\ t_i')$ instead.
The last rule is used to convert an $n$-ary equality into a
conjunction of binary equalities.  Note that we represent the
conjunction as a negated disjunction as is done in the canonical form
used by \si.

\paragraph{Distinct simplification.}
This set of rules is similar to the equality simplification rules.  It
contains axioms for the following cases.
\begin{itemize}
\item Simplify Boolean distinct-applications with more than two terms to
  \verb+false+.
\item Convert binary Boolean distinct-applications to \verb+xor+.
\item Simplify distinct-applications containing the same element multiple
  times to \verb+false+.
\end{itemize}

\begin{align}
  (distinct\ t_1 \cdots t_n) &= false
  \quad \text{where $n \geq 3$ and $t_i$ are boolean}
  \tag{distinctBool}\\
  (distinct\ t_1\ t_2) &= (xor\ t_1\ t_2)
  \quad \text{where $t_1,t_2$ are boolean}
  \tag{distinctToXor}\\
  (distinct\ \cdots t \cdots t \cdots) &= false
  \quad \text{where some $t$ occurs twice}
  \tag{distinctSame}\\
  (distinct\ t_1 \cdots t_n) &= (not\ (or\ \dots (=\ t_i\ t_j)\dots))
  \quad\text{where $n \geq 2$, $1 \leq i < j \leq n$}
  \tag{distinctBinary}
\end{align}
The last rule is used to convert distinct applications into negated
binary equalities.  In this case every term must be compared to every
other term.  For $n=2$ the $or$ is omitted.

\paragraph{Xor simplification.}
This set of rules simplifies xor with \verb+true+ or \verb+false+ and
pulls negation outside of the xor.  The first three rules apply
equally if the arguments are swapped.

\begin{align}
  (xor\ false\ t_2) &= t_2
  \tag{xorFalse}\\
  (xor\ true\ t_2) &= (not\ t_2)
  \tag{xorTrue}\\
  (xor\ (not\ t_1)\ t_2) &= (not\ (xor\ t_1\ t_2))
  \tag{xorNot}\\
  (xor\ (not\ t_1)\ (not\ t_2)) &= (xor\ t_1\ t_2)
  \tag{xorNot}\\
  (xor\ t\ t) &= false
  \tag{xorSame}
\end{align}

\paragraph{Negation simplification.}  \si simplifies negation to prevent
double negation and negations of \verb+true+ or \verb+false+.
\begin{equation}
  \begin{aligned}
    (not\ false) &= true\\
    (not\ true) &= false\\
    (not\ (not\ F)) &= F
  \end{aligned}
\tag{notSimp}
\end{equation}

\paragraph{Disjunction simplification.}  \si simplifies disjunctions by removing
duplicates or falsety and simplifying trivial tautologies to $true$.  We
justify these steps by two axioms.
\begin{align}
  (or\ \cdots true \cdots) &= true \tag{orTaut}\\
  (or\ t_1 \cdots t_n) &= (or \cdots t_i \cdots)
  \text{\begin{tabular}{l}where right hand side contains the terms in same\\
      order with duplicates and false removed\end{tabular}}
  \tag{orSimp}
\end{align}
For (orSimp), if the or would contain only one element, the right hand
side is just this element.  If the or would contain no element, the
right hand side is $false$.

\paragraph{If-then-else simplification.}  \si applies trivial Boolean
simplification to if-then-else terms where one of the arguments is
$true$ or $false$.  These simplifications are again justified by
axioms.
\begin{align}
  (ite\ true\ t_1\ t_2) &= t_1 \tag{iteTrue}\\
  (ite\ false\ t_1\ t_2) &= t_2 \tag{iteFalse}\\
  (ite\ t_0\ t\ t) &= t \tag{iteSame}\\
  (ite\ t_0\ true\ false) &= t_0 \tag{iteBool1}\\
  (ite\ t_0\ false\ true) &= (not\ t_0) \tag{iteBool2}\\
  (ite\ t_0\ true\ t_2) &= (or\ t_0\ t_2) \tag{iteBool3}\\
  (ite\ t_0\ false\ t_2) &= (not\ (or\ t_0\ (not\ t_2))) \tag{iteBool4}\\
  (ite\ t_0\ t_1\ true) &= (or\ (not\ t_0)\ t_1) \tag{iteBool5}\\
  (ite\ t_0\ t_1\ false) &= (not\ (or\ (not\ t_0)\ (not\ t_1))) \tag{iteBool6}
\end{align}

\paragraph{Removing Annotations.}  This axiom justifies the equality
between \verb+F+ and \verb+(! F :annotations)+, i.\,e., the semantic
equivalence of these terms.
\begin{equation}
  (!\ F\ :\!annotations) = F
  \tag{strip}
\end{equation}

\paragraph{Normalizing arithmetic expressions.}
\si uses a canonical form for affine linear expressions.
\[
\cf(c_1t_1 + \cdots + c_nt_n + c_0) := (+\ (*\ c_1\ t_1)\ \cdots\ (*\ c_n\ t_n)\ c_0)
\]

Here $c_i$ are rational constants in their canonical form, i.e.,
either $m$, $(-\ m)$, $(/\ m\ n)$, $(/\ (-\ m)\ n)$, with $m$ numeral
for integer arithmetic and $m,n$ decimals ending with $.0$ for real
arithmetic and if $n$ occurs, $m$ and $n$ must not have common
divisors and $n > 2$. The $t_i$ are terms not starting $+,*,/,-$ and
they may not appear twice.  For real arithmetic an integer term $t_i$
must be wrapped in $to\_real$.  The ordering of the $(*\ c_i\ t_i)$
terms is arbitrary but consistent throughout the proof.  If $c_i$ is
$1$, the multiplication is omitted.  If $c_i$ is $-1$, the
corresponding entry is just $(-\ t_i)$.  The value $c_i$ must not be
zero.  If $c_0$ is zero it is omitted unless $n=0$.  The $+$ is
omitted if it has only one argument.

The canonical form $\cf(t)$ of a term $t$ is achieved by moving
multiplicative constants before the term and moving the divisor into
the constant for division.  For addition, the constants for the same
term are added together.  Usual arithmetic on rational numbers is
performed to the constants.

As example, consider $t_1 = (+\ (*\ 3\ x)\ (*\ 2\ y))$ and $t_2 =
(+\ (*\ (- 3)\ x)\ y\ 1)$.  These terms are in canonical form. The
canonical form $\cf(t_1+t_2)$ is $(+\ (*\ 3\ y)\ 1)$ as the factors of
$x$ cancel out.  The canonical form $\cf(-t_2)$ is
$(+\ (*\ 3\ x)\ (-\ y)\ (-\ 1))$ or $(+\ (-\ y)\ (*\ 3\ x)\ (-\ 1))$
as the order of the terms $x$ and $y$ is not fixed.


\begin{equation}
  \begin{aligned}
  (+\ t_1 \cdots t_n) &= \cf(t_1+\dots+t_n)\\
  (-\ t_1 \cdots t_n) &= \cf(t_1-\dots-t_n)\\
  (*\ t_1 \cdots t_n) &= \cf(t_1 \cdots t_n)\\
  (/\ t_1 \cdots t_n) &= \cf(t_1 / (t_2\cdots t_n))\\
  (to\_real\ t_1) &= \cf(to\_real\ t_1)
  \end{aligned}
  \tag{canonicalSum}
\end{equation}

In these rules the terms $t_1,\dots,t_n$ must be already in canonical form.\\
For multiplication all but one $t_i$ must be a constant and
for division all but the first $t_i$ must be a constant.
For $to\_real$, the canonical form pushes the function $to\_real$ into
the $t_i$ and converts all integer constants to real constants.

The comparison operators are normalized to less equal with second argument 0.
\begin{align}
  (<=\ t_1\ t_2) &= (<=\ \cf(t_1-t_2)\ 0) \tag{leqToLeq0}\\
  (<\ t_1\ t_2)  &= (not\ (<=\ \cf(t_2-t_1)\ 0)) \tag{ltToLeq0}\\
  (>=\ t_1\ t_2) &= (<=\ \cf(t_2-t_1)\ 0) \tag{geqToLeq0}\\
  (>\ t_1\ t_2)  &= (not\ \cf(<=\ (t_1-t_2)\ 0)) \tag{gtToLeq0}
\end{align}

\paragraph{Arithmetic simplification.}  Arithmetic comparisons
can with constants can be trivially simplified
\begin{align}
  (<=\ c\ 0) &= true  \quad \text{where $c \leq 0$}\tag{leqTrue}\\
  (<=\ c\ 0) &= false \quad  \text{where $c > 0$}\tag{leqFalse}
\end{align}

\paragraph{Divisible rewrite.}  A divisible term
\verb+((_ divisible n) t)+ is rewritten into the equivalent term
\verb+(= t (* n (div t n)))+.  This rule can be used to
removes the divisible operator from the formula.
Furthermore, if $t$ is constant, it can be simplified.
\begin{equation}
  \begin{aligned}
    ((\_\ divisible\ 1)\ t) &= true\\
    ((\_\ divisible\ n)\ c) &= true \quad \text{where $n$ divides $c$}\\
    ((\_\ divisible\ n)\ c) &= false \quad \text{where $n$ does not divide $c$}\\
    ((\_\ divisible\ n)\ t) &= (=\ t\ (*\ n\ (div\ t\ n)))
  \end{aligned} \tag{divisible}
\end{equation}

\paragraph{Division rewrites.}  Integer division can be simplified if either
the divisor is $1$ or $-1$, or the dividend and the divisor are constant.  \si
uses different rewrite rules for these.
\begin{align}
  (div\ t\ 1) &= c \tag{div1}\\
  (div\ t\ (-\ 1)) &= \cf(-t) \tag{div-1} \\
  (div\ c_1\ c_2) &= \cf(sgn(c_2) \lfloor c_1/|c_2| \rfloor) \quad\text{where $c_2\neq 0$}\tag{divConst}
\end{align}
Here $sgn(c_2)$ stands for the signum function (1 for $c_1>0$, -1 for $c_1 < 0$).
While for positive $c_2$ the division function rounds toward negative
infinity, the SMTLIB standard specifies that $(div\ c_1\ (- c_2)) =
(-\ (div\ c_1\ c_2))$, which explains the cumbersome definition.


\paragraph{Modulo rewrites.}  This axiom justifies the rewrite of
\verb+(mod x y)+.  The result of the rewrite depends on the values of
\verb+x+ and \verb+y+.  If \verb+y+ is either $1$ or $-1$, the result is 0, if
\verb+x+ is constant, and \verb+y+ is a constant different from $0$, \si
replaces the modulo by the correct value.  Otherwise, if \verb+y+ is a
constant different from $0$, the modulo is rewritten into
\verb+(- x (* y (div x y)))+.  The rules mimic the SMTLIB semantic of the
Euclidian definition of div and mod.
\begin{align}
  (mod\ t\ 1) &= 0 \tag{div1}\\
  (mod\ t\ (- 1)) &= 0 \tag{div-1} \\
  (mod\ c_1\ c_2) &= \cf(c_1 - |c_2|\lfloor c_1/|c_2| \rfloor)
  \quad\text{where $c_2\neq 0$}
  \tag{moduloConst} \\
  (mod\ t_1\ c_2) &= \cf(t_1 - c_2 (div\ t_1\ c_2)) \quad\text{where $c_2\neq 0$} \tag{modulo}
\end{align}
For $c_2 = 0$ the rewrite rule would not be sound as per standard $mod$ must behave like an uninterpreted function.

\paragraph{Integer injection.}  The \verb+to_int+ operator can be used to
convert a real term into an integer.  The SMTLIB semantic describes this
conversion as the standard floor operation on reals.  If this is applied to a
constant, \si replaces the application by the result.
\begin{equation}
  (to\_int\ c) = \cf(\lfloor c \rfloor) \quad\text{where $c$ is constant} \tag{toInt}
\end{equation}

\paragraph{Array rewrites.}  The theory of arrays contains several constructs
that can be the target of rewrites.  \si implements the following rewrites for
arrays:
\begin{itemize}
\item Static store-over-store:  If a store overwrites the value of a directly
  adjacent store term, the inner store term is removed.
  \begin{equation}
    (store\ (store\ a\ i\ v_1)\ i\ v_2) = (store\ a\ i\ v_2)
    \tag{storeOverStore}
  \end{equation}
\item Static select-over-store: If the select-over-store axiom can be
  statically evaluated, \si replaces the application of the term by
  its conclusion.  Static evaluation can be done if the index terms
  are known to be equivalent or distinct.  Since \si does not include
  congruence closure in the simplification phase, it only simplifies
  if the index terms are syntactically equal or numerical.  For
  disequality, the difference must be a constant or, in the integer
  case, a term of the form $\sum c_it_i + c$ where $gcd(c_i)$ does not
  divide $c$.
  \begin{equation}
    \begin{aligned}
      (select\ (store\ a\ i\ v)\ i) &= v\\
      (select\ (store\ a\ t_1\ v)\ t_2) &= (select\ a\ t_2) \quad\text{where $t_1\neq t_2$ holds trivially}\\
    \end{aligned}\tag{selectOverStore}
  \end{equation}
\item Store rewrite:  In the array theory it is possible to have store terms
  that only describe values for the base array.  In this case, the store term
  can be rewritten into a select.
  \begin{equation}
    \begin{aligned}
      (=\ (store\ a\ i\ v)\ a) &= (=\ (select\ a\ i)\ v)\\
      (=\ a\ (store\ a\ i\ v)) &= (=\ (select\ a\ i)\ v)
    \end{aligned}\tag{storeRewrite}
  \end{equation}
\end{itemize}

\subsection{CNF conversion}
After normalization and simplification, \si produces CNF from the modified
input formula.  The proof rules described before justify the equivalence of
the input assertion and the canonical form.  The rules in this section justify
the transformation of the canonical form into an equisatisfiable CNF.  The
basic operations performed during this process are creation of literals from
the leaves of the Boolean part of the term, structural splitting of the
formula, and introduction of auxiliary atoms for a Plaisted--Greenbaum- or
Tseitin-style encoding of the input formula.

\paragraph{Structural splitting.}
The top-level structure of the canonical form now corresponds to a term-DAG
containing only negation, disjunction, equality between Boolean terms,
if-then-else terms, and terms corresponding to atoms.  To compute CNF from
this DAG, \si splits the DAG according to the remaining structure.  The rule
mostly used by \si splits conjunctions into one of the individual conjuncts.
The negations produced by this rule might be further simplified using
\texttt{NotSimp}.  Hence, an equality folding steps might follow directly upon
an application of this rule.

\begin{mathpar}
  \inferrule*[left=notOr,right={$j\in I$}]{(not\ (or_{i\in
      I}\ t_i))}{(not\ t_j)}\quad
\end{mathpar}

The concrete syntax for the split rule is
\begin{verbatim}
    (@split (! premise :rule) conclusion)
\end{verbatim}
where \verb+premise+ is the proof for
the premise of the rule, \verb+:rule+ is the name of the rule, and \verb+conclusion+ is the formula proved by the rule.  The following example shows the proof of $\lnot q$ from the asserted formula \verb+(not (or p q))+.

\begin{verbatim}
(@split (! (@asserted (not (or p q))) :notOr) (not q))
\end{verbatim}

Equalities between Boolean terms and if-then-else terms are not transformed by
this rule.  Note that the equalities are binary since we are dealing with
canonical form.  The rules to remove such connectives create pseudo-clauses,
i.\,e., clauses that are not actually inserted into the clause database of
\si, but further processed by rewrite rules.  The negations might be
simplified by \texttt{NotSimp} and an equality folding step.
\begin{mathpar}
  \inferrule*[left={xor+1}]{(xor\ F_1\ F_2)}{(or\ F_1\ F_2)}\quad
  \inferrule*[left={xor+2}]{(xor\ F_1\ F_2)}{(or\ (not\ F_1)\ (not\ F_2))}\\
  \inferrule*[left={xor-1}]{(not\ (xor\ F_1\ F_2))}{(or\ F_1\ (not\ F_2))}\quad
  \inferrule*[left={xor-2}]{(not\ (xor\ F_1\ F_2))}{(or\ (not\ F_1)\ F_2)}\\
  \inferrule*[left={ite+1}]{(ite\ F_1\ F_2\ F_3)}{(or\ (not\ F_1) F_2)}\quad
  \inferrule*[left={ite+2}]{(ite\ F_1\ F_2\ F_3)}{(or\ F_1 F_3)}\\
  \inferrule*[left={ite-1}]{(not\ (ite\ F_1\ F_2\ F_3))}{(or\ (not\ F_1) (not\ F_2))}\quad
  \inferrule*[left={ite-2}]{(not\ (ite\ F_1\ F_2\ F_3))}{(or\ F_1 (not\ F_3))}
\end{mathpar}

\paragraph{Building Literals.}
After splitting the term-DAG into clauses, every term in every clause is
transformed into a literal\footnote{This process sometimes is called
  \emph{internalization}.}.  This step might slightly change the term
representation due to implicit application of symmetry or associativity
axioms, or might even simplify the term to \texttt{false} if theory reasoning
allows further simplification, e.\,g., in the case of linear equalities over
the integer (e.\, g., $2x=1$).  We justify this step by another rewrite rule.
\[
\inferrule*[left=intern,right={$F\equiv F'$}]{ }{F = F'}
\]
This axiom also justifies the introduction of a proxy literal as needed by
Plaisted--Greenbaum- or Tseitin-style CNF encoding. The proxy literal captures
the truth value of the corresponding sub-formula.  The proxy literal
is named directly after the sub-formula and is used as a regular literal in the
remaining proof.  Note that \si does not produce proxy literals for
negated formulas but negates the proxy for the positive formula instead.

\paragraph{Building clauses.}
A literal in \si is represented in the proof by its SMTLIB term.
The literals may be slightly normalized by the rewrite rule (intern).

\begin{align}
  (=\ a\ b) &= (=\ b\ a)
  \tag{intern}\\
  (=\ a\ b) &= (=\ (+\ b\ (*\ (-\ 1)\ a))\ 0)
  \tag{intern}
\end{align}

When creating equality literals in congruence closure we first check for
trivial equalities:
An equality $t_1=t_2$ is trivially $true$ if $t_1$ and $t_2$ are identical.
If the sort is integer or real, it is trivially $false$ if $t_1 - t_2$ is
a non zero constant, or in the integer case if it is of the form
$\sum_{i>0} c_i t_i + c_0$ and the constant $c_0$ does not
divide the $gcd(c_1,\dots,c_n)$.
Furthermore the equality literals are normalized, i.e., the sides of the
equality may be swapped, e.g., if this results in a previously created
literal.
\begin{equation}
  (=\ t_1\ t_2) = \begin{cases} true & \text{if trivially true}\\
    false & \text{if trivially false}\\
    (=\ t_2\ t_1) & \text{if that equality was created earlier}
  \end{cases}
  \tag{intern}
\end{equation}

When creating equality literals in linear arithmetic, the first step is
to divide the affine term $\sum_{i>0} c_i t_i + c_0$ by $gcd(c_1,\dots,c_n)$, the
gcd of the coefficients of the $t_i$ terms.  For rationals, this is defined such
that dividing by the gcd results in integer coefficients without a common divisor.
The sign is normalized, so the term can also be negated.  For rationals, this
may result in a negated $<$ literal instead of a $<=$ literal.
For integer inequalities, the constant $c_0/gcd$ is then rounded to the nearest
integer such that the literal does not change; if it is an equality it is simplified
to false if $c_0/gcd$ is not an integer.  
\begin{align}
  (=\ t\ 0) &= \begin{cases} false & \text{if trivially false}\\
    (=\ \cf(t/gcd)\ 0) \\
    (=\ \cf(-t/gcd)\ 0)
  \end{cases}
  \tag{intern}\\
  (<=\ t\ 0) &= \begin{cases} 
    (<=\ \cf(t/gcd)\ 0) \\
    (not\ (<\ \cf(-t/gcd)\ 0)) &\text{(real case)}\\
    (not\ (<=\ \cf(-t/gcd + 1)\ 0))& \text{(integer case)}
  \end{cases}
  \tag{intern}
\end{align}


This rewrite rules are again applied by the equality folding rule \texttt{eq}.
The rule \texttt{clause} permutes the resulting clause.
\begin{mathpar}
\inferrule*[left=clause]{(or\ t_1\ldots t_n)}{(or\ t_i')}
\end{mathpar}
Note that we do not use $0$-ary or unary or-applications.  Instead, we write
\texttt{false} or the unit literal, respectively.

\todo{This rule now only serves the purposes of permuting a clause from its
  derivation and to visualize the clauses before they are fed into the
  resolution proof.  In my opinion, permuting is unnecessary since it does not
  change validity of the proof.  Seeing the clause before they are fed into
  the resolution proof is nice if one only wants to track the resolution
  steps.  But then why use the full proof mode?  For proper proof validation,
  seeing the clause is actually unnecessary unless the derived clause deviates
  from its derivation.  However, I expect the resolution proof to fail in this
  case as well.  So I vote for the removal of this rule since all useful
  functionality has been replaced by a combination of flatten and simpOr.}

\paragraph{Tautologies.}  For some introduced terms a corresponding tautology
is created during the transformation into CNF.  These tautologies are
clauses that are annotated with the kind of the tautology.

Table~\ref{tab:tautforms} lists the different kinds of tautologies present
in \si.  Here $F$ denotes formulas, $t$ denotes terms,
and $t_F$ denotes the term corresponding to $F$\footnote{In the proof returned
  by \si, $t_F$ and $F$ are equal.  The distinction is only done internally
  since $F$ is a Boolean term handled by the DPLL core and $t_F$ is the term
  used by congruence closure.}.
\si internally collects the disjunctive parts of the individual formulas.

The \texttt{termITE} tautology defines the meaning of non-boolean
$ite$ terms.  It can also be used for nested $ite$ terms, i.\,e., when
the then- or else-part of the ite is an ite again.  We collect a
literal for every condition $F_i$ on the path from the $ite$ term to
the subterm $t$ and include it negatively or positively depending on
whether term $t$ appears in the second or third argument of the $ite$
term.  We do not necessarily unfold all nested $ite$ terms, i.\,e., the
subterm $t$ can be another $ite$ term, e.\,g., because it occurs
multiple times in the formula.
For the \texttt{termITEBound} the $min()$ and $max()$
functions are computed at pre-processing time.  These axioms are only
instantiated if all branches in the nested ite term only differ by
constants.
\begin{table}[htbp]
  \begin{tabular}{l|l}
    Kind & Clause\\\hline
    trueNotFalse & $true\neq false$\\
    or+ & $(or\ F_1\ \ldots\ F_n)\lor \neg F_i$\\
    or- & $\neg(or\ F_1\ \ldots\ F_n)\lor F_1\lor \ldots\lor F_n$\\
    ite+1 & $(ite\ F_1\ F_2\ F_3)\lor\neg F_1\lor\neg F_2$\\
    ite+2 & $(ite\ F_1\ F_2\ F_3)\lor F_1\lor\neg F_3$\\
    ite+Red & $(ite\ F_1\ F_2\ F_3)\lor\neg F_2\lor\neg F_3$\\
    ite-1 & $\neg(ite\ F_1\ F_2\ F_3)\lor\neg F_1\lor F_2$\\
    ite-2 & $\neg(ite\ F_1\ F_2\ F_3)\lor F_1\lor F_3$\\
    ite-Red & $\neg(ite\ F_1\ F_2\ F_3)\lor F_2\lor F_3$\\
    xor+1 & $(xor\ F_1\ F_2)\lor F_1\lor\neg F_2$\\
    xor+2 & $(xor\ F_1\ F_2)\lor\neg F_1\lor F_2$\\
    xor-1 & $\neg(xor\ F_1\ F_2)\lor F_1\lor F_2$\\
    xor-2 & $\neg(xor\ F_1\ F_2)\lor\neg F_1\lor\neg F_2$\\
    excludedMiddle1 & $(=\ F\ true) \lor \neg F$\\
    excludedMiddle2 & $(=\ F\ false) \lor F$\\
    termITE & $(\neg) F_1\lor\ldots\lor(\neg) F_n \lor (ite\ F_1 (ite\ F_2 \cdots (ite\ F_n\ t\ \_)\cdots)) = t$\\
    termITEBounds & $(ite F\ (ite \dots t_1 \dots t_n)) - max(t_i) \leq 0$\\
     & $min(t_i) -(ite\ F (ite \dots t_1 \dots t_n)) \leq 0$\\
    divLow & $d\cdot (x\div d) - x \leq 0$\\
    divHigh & $\neg (|d| - x + d\cdot (x\div d)\leq 0)$\\
    toIntLow & $to\_real(to\_int(x)) - x \leq 0$\\
    toIntHigh & $\neg (1 - x + to\_real(to\_int(x))\leq 0)$\\
    eq & $c_1t_1=c_2t_2\lor
    \pm\frac{c_1}{gcd(c_1,c_2)}t_1\mp\frac{c_2}{gcd(c_1,c_2)}t_2\neq 0$ or\\
    &$c_1t_1\neq c_2t_2\lor \pm\frac{c_1}{gcd(c_1,c_2)}t_1\mp\frac{c_2}{gcd(c_1,c_2)}t_2=0$
  \end{tabular}
  \caption{\label{tab:tautforms}Different Kinds of Tautology Clauses.}
\end{table}

\subsection{Theory Lemmas}
Theory lemmas are tautological clauses that follow from one of the
theories.  In \si we have the theories of Congruence Closure, Linear
Arithmetic, and Arrays.  Furthermore there is a Nelson-Oppen theory
that translates equalities from congruence closure to linear
arithmetic.

A lemma is created by the \verb+@lemma+ function, which takes as
argument the proved clause that is annotated with some theory specific
annotations.
\begin{verbatim}
:funs ((@lemma bool @Proof))
\end{verbatim}

\paragraph{Congruence Closure}

There are only two kinds of rules for congruence closure: transitivity
and congruence.  A transitivity proves a lemma of the form ${t_1 = t_2}
\land \cdots\land {t_{n-1}= t_n} \rightarrow {t_1 = t_n}$ and is
represented by an SMTLIB term as follows:

\begin{verbatim}
(@lemma (! (or (= t1 tn)
               (not (= t1 t2)) ... (not (= tn-1 tn)))
  :CC ((= t1 tn) :subpath (t1 t2 ... tn))))
\end{verbatim}

The annotation \verb+:CC+ marks this as a congruence closure lemma.
Note that the \verb+:subpath+ annotation is part of the argument for
\verb+:CC+.  Its first item in the annotation is the proved equality,
which should occur positively in the clause.  It does not appear in
the clause if the equality is trivially false, e.g. $x = x+1$ or $2x =
2y+1$ where $x,y$ are integer.  The subpath explains why the proved
equality follows from the other equalities and for each adjacent terms
in the path the corresponding negated equality must exists in the
clause.  The literals in the clause can appear in any order, and the
arguments of the equality literals can be swapped.

The other type of lemma proves a congruence of the form $f(t_1, \dots,
t_n)\neq f(t'_1,\dots,t'_n) \lor t_1=t_1' \lor \cdots \lor t_n=t_n'$.
It is represented very similarly by the following SMTLIB term.

\begin{verbatim}
(@lemma (! (or (= (f t1...tn) (f t'1...t'n))
               (not (= t1 t'1)) ... (not (= tn t'n)))
    :CC ((= (f t1...tn) (f t'1...t'n)) 
         :subpath ((f t1...tn) (f t'1...t'n))
\end{verbatim}

Again first argument of the annotation is the proved equality, which
should occur in the clause.  The subpath contains only two
elements (transitivity always needs more than two elements).  If $t_i$
and $t'_i$ are identical, there is no corresponding negated equality
literal in the clause.  Again, the literals in the clause can appear
in any order, and the arguments of the equality literals can be
swapped.

\paragraph{Array Lemma}
\todo{cite weak equivalence paper, and const, present rules}

\begin{verbatim}
(@lemma (! (or (= (select a i) (select b j))
               (not (= i j)) ...)
  :read-over-weakeq ((= (select a i) (select b j))
                     :weakpath (i (a ... b)))))
\end{verbatim}
The negated equality $i\neq j$ is omitted if $i$ and $j$ are identical.
For any two adjacent terms in the weak path of this lemma, there must be either
(1) a corresponding negated equality in the clause, or (2) one of the term must be of
the form $(store\ a k \cdot)$ where $a$ is the other term and there is must be
a (non-negated) equality $i=k$ in the clause (i.e., the conflict guarantees $i\neq k$).
The equality $i=k$ is omitted if it is trivially false, e.g., for $k = i+1$.

\begin{verbatim}
(@lemma (! (or (= (select a i) v) ...)
  :read-const-weakeq ((= (select a i) v)
                      :weakpath (i (a ... const v)))))
\end{verbatim}
Again, for any two adjacent terms in the weak path of this lemma, there must be either
(1) a corresponding negated equality in the clause, or (2) one of the term must be of
the form $(store\ a k \cdot)$ where $a$ is the other term and there is must be
a (non-negated) equality $i=k$ in the clause (i.e., the conflict guarantees $i\neq k$).
The equality $i=k$ is omitted if it is trivially false, e.g., for $k = i+1$.
Note: For \verb+:read-const-weakeq+, if $a = (const\ v)$, the weak path
contains only one element.  This is the only reason why one can have a
path of length one.
\begin{verbatim}
(@lemma (! (or (= a b) ...)
  :weakeq-ext ((= a b)
               :subpath (a ... b)
               :weakpath (i1 (a ... b))...
               :weakpath (in (a ... b)))))
\end{verbatim}
For any two adjacent terms in the sub path of this lemma, there must be either
(1) a corresponding negated equality in the clause, or (2) one of the term must be of
the form $(store\ a i_j \cdot)$ where $a$ is the other term and there is a corresponding
weak path on index $i_j$.  For each adjecent terms $t_1,t_2$ in the weak path, there must be
(1) a corresponding negated equality in the clause, or (2) one of the term must be of
the form $(store\ a k \cdot)$ where $a$ is the other term and there is a
(non-negated) equality $i_j=k$ in the clause (i.e., the conflict guarantees $i_j\neq k$),
(3) some $i,j$ exists such that the clause contains the negated equalities
$i_k\neq i$, $i_k\neq j$, and $(select\ t_1\ i) \neq (select\ t_2\ j)$, or
(4) $t_2 = (const\ v)$ and $i$ exists such that the clause contains
$i_k\neq i$ and $(select\ t_1\ i) \neq v$ (or analogously for $t_1=(const\ v)$.
Note that in case (3) and (4) the term for which $(select t i)$ should exists, can itself
be a const term.  There is no annotation giving the terms $i,j$ in case (3) or (4); these
must be found by checking all disequalities involving $select$ terms.
Again, the equality $i=k$ is omitted if it is trivially false, e.g., for $k = i+1$ and the
disequality $i_k \neq i$ is omitted if the indices are identical.

\begin{verbatim}
(@lemma (! (or (= (const v) (const v')) ...)
  :const-weakeq ((= (const v) (const v'))
                 :subpath ((const v) ... (const v')))))
\end{verbatim}
For any two adjacent terms in the subpath of this lemma, there must be either
a corresponding negated equality in the clause, or one of the term must be of
the form $(store\ a \cdot \cdot)$, where $a$ is the other term.
\todo{This is not yet implemented exactly like this.  read-over-weakeq
  still have a strong path for i=j of length 2.  We may include some more
  trivial strong paths of length 2.}


\paragraph{Linear Arithmetic}

A linear arithmetic lemma proves the contradiction of linear
inequalites $t_1\leq 0\land \dots\land t_n\leq 0 \rightarrow false$, if
there are coefficients $c_1,\dots,c_n > 0$ such that $\sum c_it_i$ is
a negative constant.  The sum can also be zero, if at least one of the
inequalities is strict.  We also allow equalities to occur in the
conflict.

Since a theory lemma is a negated conflict, the literals occur negated
in the clause.  However a negated literal is again an inequality and
$t_1\leq 0$ can also be represented as $\neg (-t_1 <0)$, in which case
the negated literal is $(<\ -t_1\ 0)$.  In this case we use a negative
constant $c_i$, so that we can multiply $c_i$ with the term occuring
in the formula.  So if a inequality literal occurs negated in the
clause, $c_i$ must be positive, if it occurs positive in the clause
$c_i$ must be negative.  Equality literals may only occur negated in
the clause (positive in the conflict) and for them only $c_i\neq 0$ is
required.  An LA lemma is annotated by the constants $c_i$ in the same
order as the literals in the lemma.  For example if $t_1 + t_2 - t_3 =
0$, the following lemma explains why $t_1 < 0 \land t_2 = 0 \land t_3
>= 0$ is unsatisifiable.
\begin{verbatim}
(@lemma (! (or (not (< t1 0)) (not (= t2 0)) (< t3 0))
  :LA (1 1 (- 1))))))
\end{verbatim}

For integer arithmetic, if a literal occurs positively in the clause as
as $(<=\ t_i\ 0)$, one needs to take care that the negation is
$(<= (-t_i+1)\ 0)$, so the term $c_i(t_i - 1)$ is added instead
of $c_i t_i$.  This is also the case in mixed integer and real arithmetic,
if the literal contains only integer variables.

A positive equality literal can only appear in a trichotomy lemma.
This lemma explains $t \leq 0 \rightarrow t < 0 \lor t=0$ and has the form
\begin{verbatim}
(@lemma (! (or (< t 0) (= t 0) (not (<= t 0))) :trichotomy))
\end{verbatim}
The literals may appear in any order in the clause.  For integer
arithmetic the literals never use $<$ and the lemma has the form.
\begin{verbatim}
(@lemma (! (or (<= "t+1" 0) (= t 0) (not (<= t 0))) :trichotomy))
\end{verbatim}
Here \verb|"t+1"| stands for the canonical form of $t+1$.

There are no cut lemmas in \si.  Instead \si uses extended branches on
linear integer terms.  Branching is not part of any theory lemma and
is handled by the resolution proof.

\paragraph{Nelson-Oppen Combination}

These lemmas are a bit special as they are created by linear
arithmetic and by congruence closure to propagate equalities to the
other theory.  We use slightly different representation of equalities
in these theories.  As explained earlier in congruence closure an
equality $t_1=t_2$ is represented as $(=\ t_1\ t_2)$,
while in linear arithmetic it is $(=\ \cf_{gcd}(t_1-t_2)\ 0)$.
To connect these literals, we use Nelson-Oppen theory lemmas of the form.
\begin{verbatim}
(@lemma (! (or (= t1 t2) (not (= "t1-t2" 0))) :EQ))
\end{verbatim}
This propagates an equality from linear arithmetic to congruence
closure.  A similar lemma where the literals occur in the opposite
polarity is created to propagate an equality from congruence closure
to linear arithmetic.

\section{Implementation}
After presenting the set of rules used by \si, we now describe some
implementation issues that describe how to compactly represent the
proof in memory or as text.

\subsection{Proof-DAGs and letted terms}
In \si, terms are represented as DAGs, i.e., a term object like a
function application contains references to its subterms.  These
references can be shared with other functions.  This sharing can make
a proof DAG be exponentially more succinct than its corresponding
tree.  We force this sharing rigorously by only creating a term if a
syntactically identical term does not already exists.

The proof DAG can lead to complications with the $let$ operator, where
the same term variable (and thus any term containing it) can have
different meaning depending on which $let$ binds this term variable,
i.e., where in the proof DAG it appears.  This can be avoided either
by using a different term variable object for every let, or by
avoiding let altogether.  In \si we choose the latter and expand all
$let$ terms after initial parsing.  We consider the $let$ syntactic
sugar and consider different let terms that expand to the same proof
DAG to be identical.  The expansion process simply replaces any
reference to the let variable to the reference of the term that is
bound to it.  This does not create more terms, as all previous
occurences of references to the let variable are only changed to
references to the shared term.

When working on term DAGs, it is very important to keep a cache of
already visited terms and to perform the work only once for each term,
in particular, one must only iterate once through the subterms of a
term.  When printing out DAGs, we reintroduce lets for every shared
term that is not a constant.

For proof terms this means that although the simplification of a
formula and the splitting into sub-clauses is only done once, the
proof can be referenced multiple times in the proof DAG, e.g., if the
clause was used in several resolution steps.  Moreover, we often
include the same complicated term multiple times in rewrite rules,
when rewriting one of the surrounding terms in multiple steps.  These
are also represented by references to the same object, and it is
important to not iterate them more than once.

\subsection{Basic Proof Rules}

On a high level \si uses the following proof rules:
the following four proof rules introduce valid formulas:
\textsc{lemma}, \textsc{tautology}, \textsc{asserted}, \textsc{assumption}.
Lemma is used for theory lemmas.  Tautology is used by the
pre-processor to introduce auxiliary literals.  Asserted is used to
introduce formulas that were asserted in the script and assumption is
used to introduce assumption literals.  Additional we have the
following proof rules.

\[
\inferrule*[left=res]{C_1 \lor \ell \\ C_2 \lor \lnot \ell}{C_1 \lor C_2}\qquad
\inferrule*[left=eq]{t_1 \\ t_1 = t_2}{t_2}\qquad
\inferrule*[left=exists]{F = G}{\exists x F = \exists x G}
\]
\[
\inferrule*[left=trans]{t_1 = t_2 \\ t_2 = t_3}{t_1 = t_3}\qquad
\inferrule*[left=cong]{t = f(\dots t_i \dots) \\ t_i = t'_i}{t = f(\dots t'_i \dots)}\qquad
\inferrule*[left=refl]{}{t = t}
\]
\[
\inferrule*[left=split,right={where $F$ implies $G$}]{F}{G}\qquad
\inferrule*[left=rewrite,right={where $t_1 = t_2$ is a tautology}]{}{t_1 = t_2}
\]

We distinguish between tautologies that always hold and asserted
formulas that only hold in a certain context (after assert was
called).  The main reason why we distinguish between a tautology and a
lemma is the interpolator.  The tautologies were created when
simplifying a single input formula.  Their interpolants don't have to
be computed, as all literals are in the same input formula.  On the
other hand, theory lemmas require theory specific interpolation
procedures.  Rewrite rules are used together with the rules
\textsc{trans} and \textsc{cong}, to build a rewrite proof that
simplifies a complex input formula or tautology.  Reflexivity
\textsc{refl} is used very often and is a neutral element for other
proof rules like \textsc{trans}, \textsc{eq}, and the right hand side
of \textsc{cong}.  So \textsc{refl} is often only needed as marker
that we didn't rewrite the formula yet and may be later removed from
the final proof.  To quickly detect it, it deserves its own proof
rule.

\subsection{Proof Tree Structure}
The proof tree consist of three parts:
\begin{enumerate}
\item transforming input into canonical form,\label{it:tocan}
\item transforming canonical form into clausal form,\label{it:tocnf} and
\item the resolution proof\label{it:resproof}.
\end{enumerate}
The proof tree structure will correspond to this ordering.
\[
\inferrule*[Left=res,sep=1em]{
  \inferrule*[Left=Eq]{
    \inferrule*[Left=split]{
    \inferrule*[Left=Eq]{
      F_{input} \\ \text{rewrites}}
               {F_{canonical}}}{(or_{j\in J_1}\ t_j)} \\ (=\ t_j\ t_j')}{(or\ t_i)}
  \\ \ldots \\
  \inferrule*[Left=eq]{
    \inferrule*[Left=split]{
      \inferrule*[Left=Eq]{
        F_{input} \\ \text{rewrites}}
                 {F_{canonical}}}{(or_{j\in J_n}\ t_j)} \\ (=\ t_j\ t_j')}{(or\ t_i)}
  \\ \mathcal{T}\!-lemmas
}{\bot}
\]
\todo{Should we include the flatten-rule in this picture?}

Note that the conversion steps from $F_{input}$ into $F_{canonical}$
for one input formula can be referenced multiple times in the proof
DAG and will be eliminated by common subexpression elimination (let).
Only the step producing the clauses from the canonical formula will
change.  Furthermore note that we do not give a derivation for theory
lemmas.  These clauses are annotated with enough information to
reconstruct a proof of validity.

\section{Proof Terms in \si}
Proofs are represented as terms of sort \verb+@Proof+ in \si.  Note that the
\verb+@+ is part of the name of the sort to prevent unintentional clashes with
user-defined sorts since \verb+@+ and \verb+.+ are reserved for solvers in the
SMTLIB standard.

There are several rules to convert a Boolean formula into a proof.  In the
following we will show these rules and discuss their annotations.

\paragraph{Asserted terms.}  Terms asserted by the users are converted into
proof objects by the \verb+@asserted+ function.  This function has type
$Bool\rightarrow @Proof$.  The formula still contains all annotations.
Especially the \verb+:named+ annotation is kept and can be used to recognize
the input formula.

Adding specialized names to asserted formulas to simplify recognition is not a
good idea.  Naming a formula has some side-effects and leads to an application
of the \verb+strip+-rule.  While the latter might not be too bad, we can avoid
the side-effects by annotating the formula with a keyword different from
\verb+:named+ (or \verb+:pattern+, or \verb+:pat+).  This new keyword has no
meaning to the solver.  Hence it will be ignored (except for stripping it from
the formula) and can be used to recognize the input formula.

\paragraph{Rewrites.}  The most common rule is the rewrite rule, which
introduces equalities that are used to rewrite an input term.  This
rule (or rather axiom) is introduced into the proof tree by the
\verb+@rewrite+ function.  This function takes the Boolean formula
$old = new$ to rewrite all occurrences of $old$ in a formula into
$new$.  The function has signature $Bool\rightarrow @Proof$.  The
argument to the \verb+@rewrite+ function is annotated with the name of
the rewrite axiom used, which a proof checker can use to quickly check
that the application syntactically matches the rule.

A complex input formula requires several rewrite step applied to
different parts of the input formula.  The rule \verb+@cong+ can be
used to descend into a already rewritten formula to rewrite its
arguments and the rule \verb+@trans+ can be used to rewrite a formula
a second time.  To descend into a formula that has not been rewritten
yet, the rule \verb+@refl+ can be used as first argument of
\verb+@cong+.

If we rewrite a term that appears multiple times as argument of the
same function application, we will rewrite all occurrences of this
term to the same formula.  In the proof we will repeat the
\verb+@cong+ rule for each argument and when checking proofs, it does
not matter on which argument the \verb+@cong+ rule is applied first.

\paragraph{Structural Splitting.}  If the Boolean structure of the input is
more complex and cannot be transformed into disjunctive form during
canonicalization, \si uses structural splitting.  The proof tree uses the
\verb+@split+ function to describe a structural split.  This function has
signature $@Proof\times Bool\rightarrow @Proof$ where the first argument is
annotated with the rule used in this splitting step and the second argument is
the result of the split.

Splitting conjunctions is a special case here.  \si might insert another
\verb+@eq+ rewrite to rewrite double negations directly after the rewrite.
The reason for this asymmetric behavior in structural splitting is that after
splitting a conjunction, we might directly split another time on either
another (nested) conjunction, or a different operator.  Hence, if a
conjunction split produces a double negation, the next proof step will be an
equality rewrite that replaces the double negation.

\paragraph{Applying rewrites.}  The \verb+@eq+ function is used to apply
rewrites to a proof.  This function is left-associative and has signature
$@Proof\times@Proof\rightarrow @Proof$.  This signature, however, is not
strong enough to express all details.  The second proof is a proof of a
rewrite equality that is introduced with \verb+@rewrite+
while the first proof can be \verb+@asserted+ or a derivation sequence.

\paragraph{Creating literals}  After converting an input formula in canonical
form and possibly splitting it, \si creates clauses.  The input to this
process is a (possibly nested) disjunction.  For every disjunct, a literal
will be created.  This literal should replace the term in the (flattened)
clause.  We distinguish different kinds of literals, but all literals are
introduced into the proof tree using the (intern) rewrite rule with.

\paragraph{Tautologies.}  For complex formulas we create auxiliary literals.
These literals are defined by tautology clauses.  These clauses are build from
a tautology axiom which is usually already in CNF.  The \verb+@tautology+
function introduces a proof term for a tautology.  The function has
signature $Bool\rightarrow @Proof$, where the argument is the introduced
tautology.

\paragraph{Lemmas.}  The resolution proof produced by \si contains lemmas
created by theory solvers.  These lemmas are valid Boolean terms.  The
function \verb+@lemma+ transforms the clause into a proof term.  Hence, it has
signature $Bool\rightarrow @Proof$.

The lemmas are annotated with a theory identifier and further,
theory-specific annotations.  The theory identifiers currently supported are
\verb+trichotomy+, \verb+LA+, \verb+CC+, \verb+EQ+ (for Nelson-Oppen combination), and
\verb+read-over-weakeq+, \verb+read-const-weakeq+, \verb+weakeq-ext+,
and \verb+const-weakeq+.  Proofs additionally carry some information, such
as in the case of linear arithmetic the coefficients used by the solver
when applying Farkas' lemma to the input
problem.

\paragraph{Creating clauses.}  The final step in the conversion of an input
formula into CNF is the creation of clauses from disjunctive parts of the
input formula.  We get these parts by structural splitting of the input
formula that is proven by the \verb+@split+ function.

Once a (possibly nested) disjunction is created by a split \si produces a
clause for this disjunction.  First, if needed, the disjunction is flattened.
This replaces all nested disjunctions by their disjuncts.  Then, the
individual disjuncts are replaced by literals.  This step is justified by the
\verb+@eq+-rule that applies the rewrite system whose rewrite rules are stated
by (intern).   Note that (intern) might rewrite a formula
into a negated formula.  If this is the case and the resulting literal will
be negated once more, \si will add a negation simplification rewrite.
Furthermore, some disjuncts might be internalized to false\footnote{Currently
  this only happens for unsatisfiable integer equalities, e.~g., $2x=1$} and
the disjunction might contain duplicated terms due to flattening.  In these
cases, the \verb+orSimp+ rule is used to compute the resulting clause.

Since \si internally permutes the literals in a clause, a final proof step is
used to represent the resulting clause.
This step is represented by the \verb+@clause+
function in the proof tree.  This function has
signature $@Proof\times Bool\rightarrow@Proof$.
It is correct, if the first argument is a proof for a (possibly nested)
disjunction that can be flattened and reordered into the second argument.

\paragraph{Showing Results.}  The rules in SMTLIB format produce an implicit
result.  To overcome this restriction, \si appends the results of structural
splitting and clause creation to the rules as described above.

\paragraph{Compressing Proofs.}  Besides the compression possible by
exploiting left-associativity of \verb+@eq+ and \verb+@res+, we can further
compress the proof tree.  If no rewrite takes place to transform a formula
into canonical form or to create a clause, the rule \verb+@eq+ will be
suppressed.

\section{Example Proof}
In this section, we will see an except of a proof produced by \si for the
formula $2x=z \land 2y + 1 = z$ where the variables $x,y,z$ are integer
variables.  We will focus on the steps taken by \si to transform this formula
into a set of clauses and omit the resolution refutation\footnote{The
  refutation needs two lemmas from the theory of linear integer arithmetic and
  some resolution steps.  The actual proof depends on the proof transformation
  used when printing the proof.  The derivation of the clauses, however, is
  not affected by the proof transformation.}.

We assume the formula is given in SMTLIB syntax as
\begin{verbatim}
(and (= (* 2 x) z) (= (+ (* 2 y) 1) z))
\end{verbatim}

The rewrite rule (andToOr) produces the equality
\[
(and (=\ (*\ 2\ x)\ z)\ (=\ (+\ (*\ 2\ y)\ 1)\ z)) =
(not (or (not (=\ (*\ 2\ x)\ z))\ (not (=\ (+\ (*\ 2\ y)\ 1)\ z))))
\]
which is the only rewrite step needed to transform the formula into canonical
form.  The rule \texttt{Eq} is then used to rewrite the input formula
into the canonical form
\[
(not\ (or\ (not\ (=\ (*\ 2\ x)\ z))\ (not\ (=\ (+\ (*\ 2\ y)\ 1)\ z))))\tag{canonical}\label{f:canonical}.
\]

Next, \si produces two clauses from this canonical form.  The first step in
this production is structural splitting of the negated or.  We will only
present the proof for the creation of a clause from the first ``conjunct''
since the steps for the second are similar.
\[
\inferrule*[Left=Eq,sep=.1em]{
  \inferrule*[left=notOr]{(\ref{f:canonical})}{(not (not (= (*\ 2\ x) z)))}\\
  \inferrule*[left=notSimp]{ }{(not (not (= (*\ 2\ x) z))) =
    (= (*\ 2\ x)\ z)}}
           {(=\ (*\ 2\ x)\ z)}
\]

Next, the rule \texttt{intern} produces the internal representation of the
atom.  Since we have a unit, the rule \texttt{clause} will not be used to
build the unit clause, and \si only applies \texttt{Eq}.
\[
\inferrule*[left=Eq]{(=\ (*\ 2\ x)\ z) \\
  \inferrule*[left=intern]{ }{(=\ (*\ 2\ x)\ z) = (=\ (+\ (*\ (-\ 2)\ x)\ z)\
    0)}}
           {(=\ (+\ (*\ (-\ 2)\ x)\ z)\ 0)}
\]

The whole proof then uses the unit clause just created and the unit clause
created for the second ``disjunct'' to conclude unsatisfiability of this
formula in linear integer arithmetic.  As mentioned above, we do not present
the complete proof here.  Instead, we show the part of the proof that
corresponds to the previous steps.
\begin{verbatim}
(@eq
  (@split
    (!
     (@eq
      (@asserted (and (= (* 2 x) z) (= (+ (* 2 y) 1) z)))
      (@rewrite (! (= (and (= (* 2 x) z) (= (+ (* 2 y) 1) z))
          (not (or (not (= (* 2 x) z)) (not (= (+ (* 2 y) 1) z)))))
        :andToOr))
      ) :notOr)
    (not (not (= (* 2 x) z)))
  )
  (@trans
    (@rewrite (! (= (not (not (= (* 2 x) z))) (= (* 2 x) z)) :notSimp))
    (@rewrite (! (= (= (* 2 x) z) (= (+ z (* (- 2) x)) 0)) :intern))))
\end{verbatim}

\end{document}

%% Local Variables:
%% compile-command: "pdflatex -halt-on-error proof"
%% compilation-read-command: nil
%% End:
